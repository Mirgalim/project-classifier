import io
import uuid
import time
from pathlib import Path
from typing import Optional, Tuple
import gc

import pandas as pd
import numpy as np
from scipy import sparse
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import openpyxl


STORAGE_DIR = Path(__file__).parent / "storage"
STORAGE_DIR.mkdir(parents=True, exist_ok=True)

class ClassificationError(Exception):
    pass

class Timer:
    def __init__(self, name: str):
        self.name = name

    def __enter__(self):
        self.start = time.time()
        return self

    def __exit__(self, *args):
        elapsed = time.time() - self.start
        print(f"‚è±Ô∏è  {self.name}: {elapsed:.2f}s")


def _read_excel_fast(file_bytes: bytes, label: str) -> pd.DataFrame:
    with Timer(f"Reading {label}"):
        try:
            return pd.read_excel(
                io.BytesIO(file_bytes),
                engine="openpyxl",
                dtype=str,
                na_filter=False
            )
        except Exception as e:
            raise ClassificationError(f"'{label}' —Ñ–∞–π–ª—ã–≥ —É–Ω—à–∏–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π: {e}")

def _normalize_text_fast(s: pd.Series) -> pd.Series:
    return s.str.lower().str.strip().str.replace(r'\s+', ' ', regex=True)

def _ensure_columns(df: pd.DataFrame, required: list[str]) -> pd.DataFrame:
    for col in required:
        if col not in df.columns:
            df[col] = ""
    return df[required].copy()

def _create_key_text_fast(df: pd.DataFrame) -> pd.Series:
    # '–¢”©—Ä”©–ª'-–∏–π–≥ –¥–∞–≤—Ç–∞–Ω –æ—Ä—É—É–ª–∂ –∂–∏–Ω ”©–≥–Ω”© (–∞–Ω—Ö–Ω—ã "–∑”©–≤" —Å–∫—Ä–∏–ø—Ç—Ç—ç–π –∏–∂–∏–ª)
    return (
        df['–¢”©—Ä”©–ª'].astype(str) + " " +
        df['–¢”©—Ä”©–ª'].astype(str) + " " +
        df['–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª'].astype(str) + " " +
        df['–ê–Ω–≥–∏–ª–∞–ª'].astype(str) + " " +
        df['–¢–∞–π–ª–±–∞—Ä'].astype(str) + " " +
        df['–ë—Ä–µ–Ω–¥'].astype(str)
    )

def _save_xlsx_ultra_fast(df: pd.DataFrame, path: Path) -> None:
    with Timer("Saving Excel (ultra fast)"):
        # –°–æ–Ω–≥–æ–ª—Ç–æ–æ—Ä Parquet –¥–∞–≤—Ö–∞—Ä —Ö–∞–¥–≥–∞–ª–∂ –±–æ–ª–Ω–æ
        try:
            import pyarrow.parquet as pq
            import pyarrow as pa
            parquet_path = path.with_suffix('.parquet')
            table = pa.Table.from_pandas(df)
            pq.write_table(table, parquet_path)
        except ImportError:
            pass

        # –ó–∞–∞–≤–∞–ª .xlsx-—ç—ç –±–∏—á–Ω—ç
        try:
            with pd.ExcelWriter(
                path,
                engine='xlsxwriter',
                options={
                    'strings_to_numbers': False,
                    'constant_memory': True,
                    'remove_timezone': True
                }
            ) as writer:
                df.to_excel(
                    writer,
                    index=False,
                    sheet_name='Results',
                    float_format='%.3f'
                )
            return
        except Exception:
            # –¢–æ–º dataframe “Ø–µ–¥ —Ö—ç—Å—ç–≥—á–ª—ç–Ω –±–∏—á–∏—Ö fallback
            chunk_size = 10000
            first_chunk = True
            for i in range(0, len(df), chunk_size):
                chunk = df.iloc[i:i + chunk_size]
                if first_chunk:
                    chunk.to_excel(path, index=False, engine='openpyxl')
                    first_chunk = False
                else:
                    with pd.ExcelWriter(path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:
                        chunk.to_excel(writer, index=False, startrow=i+1, header=False)


def run_classification(
    sales_bytes: bytes,
    category_bytes: bytes,
    manual_bytes: Optional[bytes] = None,
    probability_threshold: float = 0.15,
    batch_size: int = 2000,
    max_features: int = 10000,
) -> Tuple[str, pd.DataFrame]:

    total_start = time.time()

    sales_df = _read_excel_fast(sales_bytes, "sales.xlsx")
    if '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä' not in sales_df.columns:
        raise ClassificationError("sales.xlsx –¥–æ—Ç–æ—Ä '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä' –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö —ë—Å—Ç–æ–π!")

    with Timer("Processing sales data"):
        sales_df['–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä'] = _normalize_text_fast(sales_df['–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä'])
        unique_products = sales_df['–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä'].dropna().unique()
        print(f"üìä Unique products: {len(unique_products)}")

    if unique_products.size == 0:
        job_id = uuid.uuid4().hex[:12]
        out_path = STORAGE_DIR / f"angilsan_{job_id}.xlsx"
        _save_xlsx_ultra_fast(sales_df, out_path)
        return job_id, sales_df

    with Timer("Reading category Excel"):
        try:
            excel_file = pd.ExcelFile(io.BytesIO(category_bytes), engine="openpyxl")
            all_sheets = {}
            for sheet_name in excel_file.sheet_names:
                try:
                    all_sheets[sheet_name] = excel_file.parse(
                        sheet_name,
                        dtype=str,
                        na_filter=False
                    )
                except Exception:
                    continue
        except Exception as e:
            raise ClassificationError(f"–ê–Ω–≥–∏–ª–ª—ã–Ω Excel-–∏–π–≥ –Ω—ç—ç–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π: {e}")

    with Timer("Processing categories"):
        needed_cols = ['–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª', '–¢”©—Ä”©–ª', '–ê–Ω–≥–∏–ª–∞–ª', '–¢–∞–π–ª–±–∞—Ä', '–ë—Ä–µ–Ω–¥', '–°–µ–≥–º–µ–Ω—Ç']
        base_cols   = ['–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª', '–¢”©—Ä”©–ª', '–ê–Ω–≥–∏–ª–∞–ª', '–¢–∞–π–ª–±–∞—Ä', '–ë—Ä–µ–Ω–¥']

        category_dfs = []
        for sheet_name, sheet_df in all_sheets.items():
            if sheet_df.empty:
                continue
            try:
                sheet_df = _ensure_columns(sheet_df, base_cols).fillna('')
                for col in base_cols:
                    sheet_df[col] = _normalize_text_fast(sheet_df[col])
                sheet_df['–°–µ–≥–º–µ–Ω—Ç'] = sheet_name
                category_dfs.append(sheet_df)
            except Exception:
                continue

        if not category_dfs:
            raise ClassificationError("–ê–Ω–≥–∏–ª–ª—ã–Ω Excel-–¥ —Ö“Ø—á–∏–Ω—Ç—ç–π sheet –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")

        category_df = pd.concat(category_dfs, ignore_index=True)
        category_df = _ensure_columns(category_df, needed_cols)
        print(f"üìä Category entries: {len(category_df)}")

    with Timer("Creating text features"):
        category_df['—Ç“Ø–ª—Ö“Ø“Ø—Ä_—Ç–µ–∫—Å—Ç'] = _create_key_text_fast(category_df)
        cat_texts = category_df['—Ç“Ø–ª—Ö“Ø“Ø—Ä_—Ç–µ–∫—Å—Ç'].values

    if cat_texts.size == 0:
        raise ClassificationError("–ê–Ω–≥–∏–ª–ª—ã–Ω —Ç–µ–∫—Å—Ç —Ö–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞.")

    with Timer("TF-IDF vectorization"):
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            min_df=1,
            max_df=0.95,
            sublinear_tf=True,
            dtype=np.float32,
            lowercase=False,      # –±–∏–¥ –∞–ª—å —Ö—ç–¥–∏–π–Ω lowercase —Ö–∏–π—Å—ç–Ω
            norm='l2',
            ngram_range=(1, 1),
            token_pattern=r'\b\w+\b'
        )

        # ‚úÖ –ó”®–í: products + categories –Ω–∏–π–ª“Ø“Ø–ª–∂ fit —Ö–∏–π–Ω—ç
        combined_texts = np.concatenate([unique_products, cat_texts], axis=0)
        tfidf_all = vectorizer.fit_transform(combined_texts)

        prod_vecs = tfidf_all[:len(unique_products)]
        cat_vecs  = tfidf_all[len(unique_products):]

        print(f"üìä Vector shapes: categories {cat_vecs.shape}, products {prod_vecs.shape}")

    with Timer("Computing similarities"):
        # –ñ–∏–∂–∏–≥ —Ö—ç–º–∂—ç—ç—Ç—ç–π “Ø–µ–¥ cosine_similarity, –∏—Ö “Ø–µ–¥ linear_kernel
        if len(unique_products) < 5000 and len(category_df) < 10000:
            similarities = cosine_similarity(prod_vecs, cat_vecs)
            best_idx = similarities.argmax(axis=1).astype(np.int32)
            best_sim = similarities.max(axis=1).astype(np.float32)
        else:
            from sklearn.metrics.pairwise import linear_kernel
            similarities = linear_kernel(prod_vecs, cat_vecs)
            best_idx = similarities.argmax(axis=1).astype(np.int32)
            best_sim = similarities.max(axis=1).astype(np.float32)

        del similarities, prod_vecs, cat_vecs
        gc.collect()

    with Timer("Building results"):
        picked = category_df.iloc[best_idx][['–ê–Ω–≥–∏–ª–∞–ª', '–¢”©—Ä”©–ª', '–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª', '–°–µ–≥–º–µ–Ω—Ç']].reset_index(drop=True)
        classified = pd.DataFrame({
            '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä': unique_products,
            '–ú–∞–≥–∞–¥–ª–∞–ª': best_sim
        })
        classified = pd.concat([classified, picked], axis=1)

    if manual_bytes is not None:
        with Timer("Applying manual overrides"):
            manual_df = _read_excel_fast(manual_bytes, "manual_fix.xlsx")
            if '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä' not in manual_df.columns:
                raise ClassificationError("manual_fix.xlsx –¥–æ—Ç–æ—Ä '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä' –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö —ë—Å—Ç–æ–π!")
            manual_df['–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä'] = _normalize_text_fast(manual_df['–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä'])

            classified = classified.merge(manual_df, on='–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä', how='left', suffixes=('', '_–≥–∞—Ä'))

            low_conf = classified['–ú–∞–≥–∞–¥–ª–∞–ª'] < float(probability_threshold)

            def non_empty(series: pd.Series) -> pd.Series:
                # "" –±–æ–ª–æ–Ω –∑”©–≤—Ö”©–Ω whitespace-–∏–π–≥ “Ø–ª —Ç–æ–æ–Ω–æ
                return series.astype(str).str.strip().ne('')

            for col in ['–ê–Ω–≥–∏–ª–∞–ª', '–¢”©—Ä”©–ª', '–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª', '–°–µ–≥–º–µ–Ω—Ç']:
                mcol = f"{col}_–≥–∞—Ä"
                if mcol in classified.columns:
                    mask = low_conf & non_empty(classified[mcol])
                    classified.loc[mask, col] = classified.loc[mask, mcol]

            classified = classified.drop(columns=[c for c in classified.columns if c.endswith('_–≥–∞—Ä')], errors='ignore')

    with Timer("Final merge"):
        final_result = sales_df.merge(classified, on='–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä', how='left')

    job_id = uuid.uuid4().hex[:12]

    total_time = time.time() - total_start
    print(f"üéâ Total processing time: {total_time:.2f} seconds")
    print(f"üìä Processed {len(unique_products)} products with {len(category_df)} categories")

    return job_id, final_result


def save_excel_file(df: pd.DataFrame, job_id: str) -> Path:
    out_path = STORAGE_DIR / f"angilsan_{job_id}.xlsx"
    _save_xlsx_ultra_fast(df, out_path)
    return out_path
