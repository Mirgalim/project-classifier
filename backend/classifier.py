import io
import re
import uuid
import time
from pathlib import Path
from typing import Optional, Tuple
import gc

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


# ====== Storage ======
STORAGE_DIR = Path(__file__).parent / "storage"
STORAGE_DIR.mkdir(parents=True, exist_ok=True)


# ====== Errors & Utils ======
class ClassificationError(Exception):
    pass


class Timer:
    def __init__(self, name: str):
        self.name = name
    def __enter__(self):
        self.start = time.time()
        return self
    def __exit__(self, *args):
        print(f"‚è±Ô∏è  {self.name}: {time.time() - self.start:.2f}s")


# ====== Heuristic regex (MN/RU mix) ======
CONSUMABLE_PAT = re.compile(
    r"(?:\b|\s)(?:–≥|–≥—Ä|–≥—Ä–∞–º|ml|–º–ª|kg|–∫–≥|—à—Ç|—à–∏—Ä—Ö—ç–≥|—É—É—Ç|–ø–∞–∫–µ—Ç|sachet|stick|—Å—Ç–∏–∫|"
    r"3–≤1|3-in-1|mix|–∫–∞–ø—É—á–∏–Ω–æ|–ª–∞—Ç—Ç–µ|—ç—Å–ø—Ä–µ—Å—Å–æ|espresso|instant|classic|gold|"
    r"jacobs|nescafe|maccoffee|tea|—á–∞–π|–ø–∞–∫–µ—Ç–∏–∫)(?:\b|\s)"
)
APPLIANCE_PAT = re.compile(
    r"(?:–∫–æ—Ñ–µ\s?—á–∞–Ω–∞–≥—á|–∫–æ—Ñ–µ\s?–º–∞—à–∏–Ω|–∫–æ—Ñ–µ–º–∞—à–∏–Ω|—ç–ª–µ–∫—Ç—Ä–æ|—Ü–∞—Ö–∏–ª–≥–∞–∞–Ω|–≥–∞–ª —Ç–æ–≥–æ–æ–Ω—ã —Ü–∞—Ö–∏–ª–≥–∞–∞–Ω|"
    r"—á–∞–π–Ω–∏–∫|kettle|–º–∞—à–∏–Ω|–≤–∞—Ç—Ç|w\b|–í—Ç\b)"
)

# Quick keyword hints for UNCLASSIFIED fallback
KW = {
    "coffee": re.compile(r"\b(–∫–æ—Ñ–µ|–∫–∞–ø—É—á–∏–Ω–æ|–ª–∞—Ç—Ç–µ|—ç—Å–ø—Ä–µ—Å—Å–æ|espresso|3–≤1|3-in-1)\b", re.I),
    "tea":    re.compile(r"\b(—Ü–∞–π|tea|chai|–º–∞—Å–∞–ª–∞)\b", re.I),
    "beer":   re.compile(r"\b(–ø–∏–≤–æ|beer|–∞–π—Ä–∞–≥)\b", re.I),
    "water":  re.compile(r"\b(—É—Å|water)\b", re.I),
    "juice":  re.compile(r"\b(–∂“Ø“Ø—Å|juice)\b", re.I),
    "choco":  re.compile(r"\b(—à–æ–∫–æ–ª–∞–¥|choco|kinder|nestle|snickers|mars)\b", re.I),
    "candy":  re.compile(r"\b(—á–∏—Ö—ç—Ä|lollipop|trolli|candy|sour)\b", re.I),
}


# ====== Helpers ======
def _read_excel_fast(file_bytes: bytes, label: str) -> pd.DataFrame:
    with Timer(f"Reading {label}"):
        try:
            return pd.read_excel(
                io.BytesIO(file_bytes),
                engine="openpyxl",
                dtype=str,
                na_filter=False
            )
        except Exception as e:
            raise ClassificationError(f"'{label}' —Ñ–∞–π–ª—ã–≥ —É–Ω—à–∏–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π: {e}")


def _normalize_text_fast(s: pd.Series) -> pd.Series:
    return (
        s.astype(str)
         .str.lower()
         .str.strip()
         .str.replace(r"\s+", " ", regex=True)
    )


def _ensure_columns(df: pd.DataFrame, required: list[str]) -> pd.DataFrame:
    for col in required:
        if col not in df.columns:
            df[col] = ""
    return df[required].copy()


def _create_key_text_fast(df: pd.DataFrame) -> pd.Series:
    # '–¢”©—Ä”©–ª'-–∏–π–≥ —Ö–æ—ë—Ä –¥–∞–≤—Ç–∞–∂ –∂–∏–Ω ”©–≥–Ω”© (–∞–Ω—Ö–Ω—ã –ª–æ–≥–∏–∫—Ç–æ–π –∏–∂–∏–ª)
    return (
        df["–¢”©—Ä”©–ª"].astype(str) + " " +
        df["–¢”©—Ä”©–ª"].astype(str) + " " +
        df["–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª"].astype(str) + " " +
        df["–ê–Ω–≥–∏–ª–∞–ª"].astype(str) + " " +
        df["–¢–∞–π–ª–±–∞—Ä"].astype(str) + " " +
        df["–ë—Ä–µ–Ω–¥"].astype(str)
    )


def _word_count(series: pd.Series) -> pd.Series:
    return series.str.replace(r"[^\w\s]+", " ", regex=True).str.split().map(
        lambda x: len(x) if isinstance(x, list) else 0
    )


def _category_hint_tags(row: pd.Series) -> tuple[str, bool, bool]:
    text = " ".join([
        str(row.get("–¢”©—Ä”©–ª", "")),
        str(row.get("–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª", "")),
        str(row.get("–ê–Ω–≥–∏–ª–∞–ª", "")),
        str(row.get("–°–µ–≥–º–µ–Ω—Ç", "")),
    ]).lower()
    is_appliance = bool(APPLIANCE_PAT.search(text))
    is_consumable = ("–∫–æ—Ñ–µ" in text) or ("–Ω–∞–π—Ä—É—É–ª–¥–∞–≥" in text) or ("—É—É—Ö" in text) or ("beverage" in text)
    tags = []
    if is_appliance:
        tags.append("tag_appliance electronics kitchen_appliance")
    if is_consumable:
        tags.append("tag_consumable beverage coffee instant")
    return (" ".join(tags), is_consumable, is_appliance)


def _product_hint_tags(name: str) -> tuple[str, bool, bool]:
    is_consumable = bool(CONSUMABLE_PAT.search(name))
    is_appliance  = bool(APPLIANCE_PAT.search(name))
    tags = []
    if is_consumable:
        tags.append("tag_consumable beverage coffee instant –ø–∞–∫–µ—Ç —É—É—Ç –≥—Ä –º–ª")
    if is_appliance:
        tags.append("tag_appliance electronics kitchen_appliance")
    return (" ".join(tags), is_consumable, is_appliance)


def _fallback_guess(name: str) -> Optional[tuple[str, str, str]]:
    if KW["coffee"].search(name):
        return ("–ù–∞–π—Ä—É—É–ª–¥–∞–≥ –∫–æ—Ñ–µ", "–ö–æ—Ñ–µ", "–±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω —Ö“Ø–Ω—Å")
    if KW["tea"].search(name):
        return ("–¶–∞–π", "–¶–∞–π", "–±–æ–ª–æ–≤—Å—Ä—É—É–ª—Å–∞–Ω —Ö“Ø–Ω—Å")
    if KW["beer"].search(name):
        return ("–ü–∏–≤–æ", "–°–æ–≥—Ç—É—É—Ä—É—É–ª–∞—Ö —É–Ω–¥–∞–∞", "–®–∏–Ω–≥—ç–Ω —Ö“Ø–Ω—Å")
    if KW["water"].search(name):
        return ("–£—Å", "–£–Ω–¥–∞–∞", "–®–∏–Ω–≥—ç–Ω —Ö“Ø–Ω—Å")
    if KW["juice"].search(name):
        return ("–ñ“Ø“Ø—Å", "–£–Ω–¥–∞–∞", "–®–∏–Ω–≥—ç–Ω —Ö“Ø–Ω—Å")
    if KW["choco"].search(name):
        return ("–®–æ–∫–æ–ª–∞–¥", "–ß–∏—Ö—ç—Ä", "–ê–º—Ç—Ç–∞–Ω")
    if KW["candy"].search(name):
        return ("–®–∏–π—Ç—ç–Ω/–õ–æ–ª–ª–∏–ø–æ–ø", "–ß–∏—Ö—ç—Ä", "–ê–º—Ç—Ç–∞–Ω")
    return None


def _save_xlsx_ultra_fast(df: pd.DataFrame, path: Path) -> None:
    with Timer("Saving Excel"):
        # –°–æ–Ω–≥–æ–ª—Ç–æ–æ—Ä Parquet —Ö–∞–¥–≥–∞–ª–Ω–∞
        try:
            import pyarrow.parquet as pq
            import pyarrow as pa
            pq.write_table(pa.Table.from_pandas(df), path.with_suffix(".parquet"))
        except Exception:
            pass

        # –ó–∞–∞–≤–∞–ª .xlsx –±–∏—á–∏—Ö
        try:
            with pd.ExcelWriter(
                path, engine="xlsxwriter",
                options={"strings_to_numbers": False, "constant_memory": True, "remove_timezone": True}
            ) as w:
                df.to_excel(w, index=False, sheet_name="Results", float_format="%.3f")
            return
        except Exception:
            # –¢–æ–º DF “Ø–µ–¥ —Ö—ç—Å—ç–≥—á–ª—ç–Ω –±–∏—á–∏—Ö fallback
            chunk = 10000
            first = True
            for i in range(0, len(df), chunk):
                part = df.iloc[i:i+chunk]
                if first:
                    part.to_excel(path, index=False, engine="openpyxl")
                    first = False
                else:
                    with pd.ExcelWriter(path, engine="openpyxl", mode="a", if_sheet_exists="overlay") as w:
                        part.to_excel(w, index=False, startrow=i+1, header=False)


# ====== Main ======
def run_classification(
    sales_bytes: bytes,
    category_bytes: bytes,
    manual_bytes: Optional[bytes] = None,
    probability_threshold: float = 0.15,
    batch_size: int = 2000,     # reserved; not used in this variant
    max_features: int = 10000,
) -> Tuple[str, pd.DataFrame]:

    total_start = time.time()

    # --- Sales
    sales_df = _read_excel_fast(sales_bytes, "sales.xlsx")
    if "–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä" not in sales_df.columns:
        raise ClassificationError("sales.xlsx –¥–æ—Ç–æ—Ä '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä' –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö —ë—Å—Ç–æ–π!")

    with Timer("Processing sales data"):
        sales_df["–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä"] = _normalize_text_fast(sales_df["–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä"])
        unique_products = sales_df["–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä"].dropna().unique()
        print(f"üìä Unique products: {len(unique_products)}")

    if unique_products.size == 0:
        job_id = uuid.uuid4().hex[:12]
        _save_xlsx_ultra_fast(sales_df, STORAGE_DIR / f"angilsan_{job_id}.xlsx")
        return job_id, sales_df

    # --- Categories
    with Timer("Reading category Excel"):
        try:
            excel_file = pd.ExcelFile(io.BytesIO(category_bytes), engine="openpyxl")
            all_sheets = {}
            for sheet_name in excel_file.sheet_names:
                try:
                    all_sheets[sheet_name] = excel_file.parse(sheet_name, dtype=str, na_filter=False)
                except Exception:
                    continue
        except Exception as e:
            raise ClassificationError(f"–ê–Ω–≥–∏–ª–ª—ã–Ω Excel-–∏–π–≥ –Ω—ç—ç–∂ —á–∞–¥—Å–∞–Ω–≥“Ø–π: {e}")

    with Timer("Processing categories"):
        needed = ["–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª", "–¢”©—Ä”©–ª", "–ê–Ω–≥–∏–ª–∞–ª", "–¢–∞–π–ª–±–∞—Ä", "–ë—Ä–µ–Ω–¥", "–°–µ–≥–º–µ–Ω—Ç"]
        base   = ["–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª", "–¢”©—Ä”©–ª", "–ê–Ω–≥–∏–ª–∞–ª", "–¢–∞–π–ª–±–∞—Ä", "–ë—Ä–µ–Ω–¥"]

        cat_parts = []
        for sheet_name, sheet_df in all_sheets.items():
            if sheet_df.empty:
                continue
            try:
                sheet_df = _ensure_columns(sheet_df, base).fillna("")
                for col in base:
                    sheet_df[col] = _normalize_text_fast(sheet_df[col])
                sheet_df["–°–µ–≥–º–µ–Ω—Ç"] = sheet_name
                cat_parts.append(sheet_df)
            except Exception:
                continue

        if not cat_parts:
            raise ClassificationError("–ê–Ω–≥–∏–ª–ª—ã–Ω Excel-–¥ —Ö“Ø—á–∏–Ω—Ç—ç–π sheet –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")

        category_df = pd.concat(cat_parts, ignore_index=True)
        category_df = _ensure_columns(category_df, needed)
        print(f"üìä Category entries (raw): {len(category_df)}")

    # --- Build texts + drop weak/empty rows
    with Timer("Creating text features"):
        base_text = _create_key_text_fast(category_df).str.strip()
        category_df["—Ç“Ø–ª—Ö“Ø“Ø—Ä_—Ç–µ–∫—Å—Ç"] = base_text

        # –•—ç—Ç —Å—É–ª/—Ö–æ–æ—Å–æ–Ω –º”©—Ä“Ø“Ø–¥–∏–π–≥ drop (>= 2 “Ø–≥—Ç—ç–π–≥ “Ø–ª–¥—ç—ç–Ω—ç)
        valid_mask = _word_count(category_df["—Ç“Ø–ª—Ö“Ø“Ø—Ä_—Ç–µ–∫—Å—Ç"]) >= 2
        dropped = (~valid_mask).sum()
        if dropped:
            print(f"üßπ Dropped empty/weak category rows: {dropped}")
        category_df = category_df.loc[valid_mask].reset_index(drop=True)

        # –•—ç—Ä–≤—ç—ç –±“Ø–≥–¥ —É–Ω–∞–≤–∞–ª –∑–æ–≥—Å–æ–æ–Ω–æ
        if category_df.empty:
            raise ClassificationError("–ê–Ω–≥–∏–ª–ª—ã–Ω —Ç–µ–∫—Å—Ç“Ø“Ø–¥ —Ö–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞ (–±—É—Å–∞–¥ sheet-“Ø“Ø–¥–∏–π–≥ —à–∞–ª–≥–∞–Ω–∞ —É—É).")

        # Domain hint flags for categories
        tags, cat_is_cons, cat_is_appl = [], [], []
        for _, row in category_df.iterrows():
            t, ccons, cappl = _category_hint_tags(row)
            tags.append(t)
            cat_is_cons.append(ccons)
            cat_is_appl.append(cappl)
        category_df["__hint_tags"] = tags
        category_df["__is_consumable"] = np.array(cat_is_cons, dtype=bool)
        category_df["__is_appliance"]  = np.array(cat_is_appl, dtype=bool)

        # Final category text
        cat_texts = (category_df["—Ç“Ø–ª—Ö“Ø“Ø—Ä_—Ç–µ–∫—Å—Ç"] + " " + category_df["__hint_tags"].fillna("")).values

        # Product augmented texts + flags
        prod_aug = []
        prod_is_cons = np.zeros(len(unique_products), dtype=bool)
        prod_is_appl = np.zeros(len(unique_products), dtype=bool)
        for i, name in enumerate(unique_products):
            t, ccons, cappl = _product_hint_tags(name)
            prod_aug.append(f"{name} {t}".strip())
            prod_is_cons[i] = ccons
            prod_is_appl[i] = cappl

    # --- TF-IDF
    with Timer("TF-IDF vectorization"):
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            min_df=1,
            max_df=0.95,
            sublinear_tf=True,
            dtype=np.float32,
            lowercase=False,
            norm="l2",
            ngram_range=(1, 2),            # 1‚Äì2 –≥—Ä–∞–º: '–∫–æ—Ñ–µ —á–∞–Ω–∞–≥—á' –≥—ç—Ö –º—ç—Ç –Ω–∏–π–ª–º—ç–ª —Ö—ç–ª–ª—ç–≥ –±–∞—Ä–∏–Ω–∞
            token_pattern=r"\b\w+\b",
        )
        combined_texts = np.concatenate([np.array(prod_aug, dtype=object), cat_texts], axis=0)
        tfidf_all = vectorizer.fit_transform(combined_texts)
        prod_vecs = tfidf_all[:len(unique_products)]
        cat_vecs  = tfidf_all[len(unique_products):]
        print(f"üìä Vector shapes: categories {cat_vecs.shape}, products {prod_vecs.shape}")

    # --- Similarity (+ domain penalty)
    with Timer("Computing similarities"):
        similarities = cosine_similarity(prod_vecs, cat_vecs)

        # Consumable product ‚ü∑ Appliance category = penalty (40% down)
        rows = np.where(prod_is_cons)[0]
        cols = np.where(category_df["__is_appliance"].values)[0]
        if rows.size and cols.size:
            similarities[np.ix_(rows, cols)] *= 0.6

        # (optionally) Appliance product ‚ü∑ Consumable category penalty
        rows2 = np.where(prod_is_appl)[0]
        cols2 = np.where(category_df["__is_consumable"].values)[0]
        if rows2.size and cols2.size:
            similarities[np.ix_(rows2, cols2)] *= 0.6

        best_idx = similarities.argmax(axis=1).astype(np.int32)
        best_sim = similarities.max(axis=1).astype(np.float32)

        del similarities, prod_vecs, cat_vecs
        gc.collect()

    # --- Build results
    with Timer("Building results"):
        picked = category_df.iloc[best_idx][["–ê–Ω–≥–∏–ª–∞–ª", "–¢”©—Ä”©–ª", "–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª", "–°–µ–≥–º–µ–Ω—Ç"]].reset_index(drop=True)
        classified = pd.DataFrame({
            "–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä": unique_products,
            "–ú–∞–≥–∞–¥–ª–∞–ª": best_sim
        })
        classified = pd.concat([classified, picked], axis=1)

        # 0-–∏–∂–∏–ª—Ç—Ç—ç–π (—ç—Å–≤—ç–ª –º–∞—à –æ–π—Ä) ‚Üí UNCLASSIFIED + optional keyword guess
        zero_mask = best_sim <= 1e-8
        if zero_mask.any():
            print(f"‚ö†Ô∏è No-match products: {zero_mask.sum()}")
            classified.loc[zero_mask, ["–ê–Ω–≥–∏–ª–∞–ª", "–¢”©—Ä”©–ª", "–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª", "–°–µ–≥–º–µ–Ω—Ç"]] = [
                "UNCLASSIFIED", "UNCLASSIFIED", "UNCLASSIFIED", ""
            ]
            # –ñ–∏–∂–∏–≥ –¥“Ø—Ä–º–∏–π–Ω fallback (—Å–æ–Ω–≥–æ–ª—Ç–æ–æ—Ä)
            for i in np.where(zero_mask)[0]:
                name = unique_products[i]
                guess = _fallback_guess(name)
                if guess:
                    a, t, e = guess
                    classified.loc[i, ["–ê–Ω–≥–∏–ª–∞–ª", "–¢”©—Ä”©–ª", "–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª"]] = [a, t, e]

    # --- Manual overrides
    if manual_bytes is not None:
        with Timer("Applying manual overrides"):
            manual_df = _read_excel_fast(manual_bytes, "manual_fix.xlsx")
            if "–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä" not in manual_df.columns:
                raise ClassificationError("manual_fix.xlsx –¥–æ—Ç–æ—Ä '–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä' –±–∞–≥–∞–Ω–∞ –±–∞–π—Ö —ë—Å—Ç–æ–π!")
            manual_df["–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä"] = _normalize_text_fast(manual_df["–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä"])

            classified = classified.merge(manual_df, on="–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä", how="left", suffixes=("", "_–≥–∞—Ä"))

            low_conf = classified["–ú–∞–≥–∞–¥–ª–∞–ª"] < float(probability_threshold)

            def non_empty(s: pd.Series) -> pd.Series:
                return s.astype(str).str.strip().ne("")

            for col in ["–ê–Ω–≥–∏–ª–∞–ª", "–¢”©—Ä”©–ª", "–ï—Ä”©–Ω—Ö–∏–π –∞–Ω–≥–∏–ª–∞–ª", "–°–µ–≥–º–µ–Ω—Ç"]:
                mcol = f"{col}_–≥–∞—Ä"
                if mcol in classified.columns:
                    mask = low_conf & non_empty(classified[mcol])
                    classified.loc[mask, col] = classified.loc[mask, mcol]

            classified = classified.drop(columns=[c for c in classified.columns if c.endswith("_–≥–∞—Ä")], errors="ignore")

    # --- Final merge
    with Timer("Final merge"):
        final_result = sales_df.merge(classified, on="–ë–∞—Ä–∞–∞–Ω—ã –Ω—ç—Ä", how="left")

    job_id = uuid.uuid4().hex[:12]
    print(f"üéâ Total processing time: {time.time() - total_start:.2f}s")
    print(f"üìä Processed {len(unique_products)} products with {len(category_df)} categories")

    return job_id, final_result


def save_excel_file(df: pd.DataFrame, job_id: str) -> Path:
    out_path = STORAGE_DIR / f"angilsan_{job_id}.xlsx"
    _save_xlsx_ultra_fast(df, out_path)
    return out_path
